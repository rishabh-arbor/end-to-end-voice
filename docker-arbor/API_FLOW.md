# Interview Bot - Complete API Flow

## Overview
This document describes the complete API flow from Docker container startup to real-time conversation between the bot and Umi (the interviewer).

---

## 1. Container Startup & Initialization

### 1.1 Docker Entrypoint (`docker-entrypoint.sh`)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Start Xvfb (virtual display)                         â”‚
â”‚ 2. Start PulseAudio daemon                              â”‚
â”‚ 3. Create virtual_speaker (sink)                        â”‚
â”‚ 4. Create virtual_mic (source from virtual_speaker)     â”‚
â”‚ 5. Start VNC server (port 5900)                         â”‚
â”‚ 6. Start noVNC web server (port 6080)                  â”‚
â”‚ 7. Execute main application (src/index.js)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**PulseAudio Setup:**
- **virtual_speaker**: Audio sink (where TTS audio is played)
- **virtual_mic**: Audio source (monitors virtual_speaker, what browser captures)
- **module-remap-source**: Creates virtual_mic from virtual_speaker.monitor

---

## 2. Application Initialization (`src/index.js`)

### 2.1 Main Flow
```
main()
  â”œâ”€> initializeLogger()
  â”œâ”€> validateConfig()
  â”œâ”€> initializeHealthServer() â†’ HTTP server on port 3000
  â”œâ”€> setupTimeout() â†’ Auto-shutdown after timeout
  â”œâ”€> initializeBrowser() â†’ Launch Chromium via Puppeteer
  â”œâ”€> setupAudioDevices() â†’ Grant audio permissions
  â”œâ”€> navigateToInterview() â†’ Navigate to interview URL
  â”œâ”€> initializeLLMClient() â†’ Create Gemini WebSocket client
  â”œâ”€> initializeConversation() â†’ Create conversation manager
  â””â”€> injectAutomation() â†’ Inject browser automation script
```

### 2.2 Health Server
- **Port**: 3000
- **Endpoints**:
  - `GET /health` â†’ Health check status
  - `GET /ready` â†’ Readiness status

---

## 3. Browser Launch (`src/browser/puppeteer-launcher.js`)

### 3.1 Chromium Configuration
```
Launch Chromium with:
  - Headless: false (for VNC viewing)
  - Audio: Enabled
  - WebRTC: Enabled
  - Flags:
    --use-fake-ui-for-media-stream (auto-grant permissions)
    --disable-features=WebRtcAecDump,AudioServiceOutOfProcess
    --disable-rtc-smoothness-algorithm
    --disable-webrtc-hw-encoding
    --disable-webrtc-hw-decoding
```

---

## 4. Page Navigation (`src/browser/page-controller.js`)

### 4.1 Navigation Flow
```
navigateToInterview()
  â”œâ”€> setupConsoleForwarding() â†’ Forward console.log to Node.js
  â”œâ”€> setupErrorHandling() â†’ Forward page errors
  â””â”€> page.goto(url) â†’ Navigate to interview URL
```

### 4.2 Audio Device Setup
```
setupAudioDevices()
  â””â”€> page.grantPermissions(['microphone', 'camera'])
      â†’ Grant permissions for interview domain
```

---

## 5. Script Injection (`src/browser/page-controller.js`)

### 5.1 Injection Process
```
injectAutomation()
  â”œâ”€> Read injected-automation.js file
  â”œâ”€> Replace placeholders:
  â”‚   - __PASSWORD__ â†’ config.interview.password
  â”‚   - __GEMINI_API_KEY__ â†’ config.gemini.apiKey
  â”œâ”€> Expose Node.js bridge functions:
  â”‚   - window.__arborLog() â†’ Log from browser to Node.js
  â”‚   - window.__arborPlayAudio() â†’ Play TTS via paplay
  â”‚   - window.__arborPlayTTSToWebRTC() â†’ Inject TTS into WebRTC
  â””â”€> page.evaluate() â†’ Execute script in browser context
```

---

## 6. Browser-Side Automation (`scripts/injected-automation.js`)

### 6.1 Initialization
```
Script starts:
  â”œâ”€> Auto-click automation (Get Started, Continue buttons)
  â”œâ”€> Fill password field
  â”œâ”€> Select language (English)
  â”œâ”€> Intercept getUserMedia() â†’ Force virtual_mic device
  â”œâ”€> Intercept RTCPeerConnection â†’ Capture WebRTC tracks
  â””â”€> Wait for interview to be ready
```

### 6.2 Audio Capture Setup
```
startAudioCapture()
  â”œâ”€> navigator.mediaDevices.getUserMedia({ audio: true })
  â”‚   â””â”€> Intercepted â†’ Forces virtual_mic device
  â”œâ”€> Create AudioContext (sampleRate: 44100)
  â”œâ”€> Create AudioWorkletNode (capture-processor.js)
  â”œâ”€> Connect microphone stream â†’ Worklet
  â””â”€> Worklet processes audio â†’ PCM â†’ Base64 â†’ Send to STT
```

**Audio Flow (Capture):**
```
Umi speaks â†’ Browser WebRTC captures â†’ virtual_mic (PulseAudio)
  â†’ AudioContext â†’ AudioWorklet â†’ PCM (16-bit, 44100Hz)
  â†’ Base64 encode â†’ Send to Gemini STT WebSocket
```

---

## 7. Gemini API Connections

### 7.1 STT (Speech-to-Text) WebSocket
```
Location: scripts/injected-automation.js

Connection:
  ws://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1alpha.GenerativeService.BidiGenerateContent?key=API_KEY

Setup Message:
{
  "setup": {
    "model": "models/gemini-2.0-flash-live-001",
    "generation_config": {
      "response_modalities": ["TEXT"],  // STT only, no audio response
      "input_audio_transcription": {}
    }
  }
}

Audio Message Format:
{
  "audio": {
    "data": "<base64-encoded-PCM>",
    "format": "pcm",
    "sampleRate": 44100
  }
}

Response:
{
  "serverContent": {
    "inputTranscription": {
      "text": "transcribed text here"
    }
  }
}
```

### 7.2 TTS (Text-to-Speech) WebSocket
```
Location: scripts/injected-automation.js

Connection: Same endpoint as STT, but separate WebSocket instance

Setup Message:
{
  "setup": {
    "model": "models/gemini-2.0-flash-live-001",
    "generation_config": {
      "response_modalities": ["AUDIO", "TEXT"],
      "speech_config": {
        "voice_config": {
          "prebuilt_voice_config": {
            "voice_name": "Puck"
          }
        }
      }
    }
  }
}

Text Message Format:
{
  "text": "Response text to convert to speech"
}

Response:
{
  "serverContent": {
    "modelTurn": {
      "parts": [
        {
          "inlineData": {
            "mimeType": "audio/pcm",
            "data": "<base64-encoded-PCM>"
          }
        }
      ]
    }
  }
}
```

---

## 8. Conversation Flow

### 8.1 Umi Speaks â†’ Bot Listens
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Umi speaks in browser                                 â”‚
â”‚ 2. Browser WebRTC captures audio from virtual_mic        â”‚
â”‚ 3. AudioWorklet processes audio chunks (2s each)         â”‚
â”‚ 4. PCM â†’ Base64 â†’ Send to STT WebSocket                 â”‚
â”‚ 5. Gemini transcribes â†’ Returns text                    â”‚
â”‚ 6. Script logs: "ğŸ“ Interviewer: <transcription>"        â”‚
â”‚ 7. Conversation manager accumulates transcript          â”‚
â”‚ 8. After 5s silence â†’ Generate response                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 Bot Responds â†’ Umi Hears
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Conversation manager creates response prompt         â”‚
â”‚ 2. Send text to TTS WebSocket                           â”‚
â”‚ 3. Gemini generates audio response (PCM, 24000Hz)      â”‚
â”‚ 4. Script receives base64 audio chunks                 â”‚
â”‚ 5. Queue audio chunks for sequential playback          â”‚
â”‚ 6. Play via paplay â†’ virtual_speaker                   â”‚
â”‚ 7. PulseAudio loopback: virtual_speaker â†’ virtual_mic  â”‚
â”‚ 8. Browser WebRTC captures from virtual_mic            â”‚
â”‚ 9. Umi hears the response                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Audio Flow (Playback):**
```
Gemini TTS â†’ Base64 PCM (24000Hz) â†’ Decode â†’ paplay
  â†’ virtual_speaker (PulseAudio sink)
  â†’ virtual_speaker.monitor (PulseAudio source)
  â†’ virtual_mic (remap-source)
  â†’ Browser WebRTC input
  â†’ Umi hears response
```

---

## 9. Conversation Manager (`src/llm/conversation.js`)

### 9.1 State Management
```
State Variables:
  - transcriptBuffer: Accumulates interviewer speech
  - isWaitingForResponse: Prevents duplicate responses
  - isTTSPlaying: Prevents echo from speaker
  - history: Conversation turn history
  - responseTimer: Triggers response after silence
  - cooldownTimer: Prevents immediate re-response
```

### 9.2 Turn-Taking Logic
```
1. Interviewer speaks â†’ Transcription received
2. Reset responseTimer
3. Accumulate transcriptBuffer
4. After 5s silence â†’ responseTimer fires
5. Generate response â†’ Send to TTS WebSocket
6. TTS audio received â†’ Play via paplay
7. After playback â†’ 15s cooldown
8. Ready for next question
```

### 9.3 Response Generation
```
generateResponse(question)
  â”œâ”€> createResponsePrompt(question)
  â”‚   â””â”€> Returns: "Based on the interviewer's question: \"{question}\", 
  â”‚                 please provide a concise and natural interview response."
  â”œâ”€> llmClient.sendText(prompt)
  â””â”€> Wait for audio response
```

---

## 10. Audio Playback (`scripts/injected-automation.js`)

### 10.1 Playback Queue
```
playAudioBase64(base64Data, sampleRate)
  â”œâ”€> Push to playbackQueue
  â””â”€> If not playing â†’ playNextFromQueue()

playNextFromQueue()
  â”œâ”€> Decode base64 â†’ PCM buffer
  â”œâ”€> PRIMARY: paplay â†’ virtual_speaker
  â”‚   â””â”€> window.__arborPlayAudio(pcmData, sampleRate)
  â”‚       â†’ Node.js spawns: paplay --device=virtual_speaker
  â”œâ”€> FALLBACK: WebRTC injection (if paplay fails)
  â”‚   â””â”€> window.__arborPlayTTSToWebRTC(pcmData, sampleRate)
  â”‚       â†’ Replace WebRTC audio track with TTS audio
  â””â”€> After playback â†’ Continue queue or start cooldown
```

### 10.2 Node.js Bridge (`src/browser/page-controller.js`)
```
window.__arborPlayAudio exposed via:
  page.exposeFunction('__arborPlayAudio', async (base64Data, sampleRate) => {
    const pcmBuffer = Buffer.from(base64Data, 'base64');
    const child = spawn('paplay', [
      '--device=virtual_speaker',
      '--rate=' + sampleRate,
      '--format=s16le',
      '--channels=1'
    ]);
    child.stdin.write(pcmBuffer);
    child.stdin.end();
  });
```

---

## 11. WebRTC Interception (`scripts/injected-automation.js`)

### 11.1 getUserMedia Interception
```
Original: navigator.mediaDevices.getUserMedia()
Intercepted to:
  1. Enumerate devices â†’ Find virtual_mic
  2. Force deviceId: { exact: virtual_mic.deviceId }
  3. Disable audio processing:
     - echoCancellation: false
     - noiseSuppression: false
     - autoGainControl: false
  4. Call original getUserMedia with modified constraints
  5. Capture audio track for potential replacement
```

### 11.2 RTCPeerConnection Interception
```
Intercept:
  - RTCPeerConnection.prototype.addTrack
  - RTCPeerConnection.prototype.addTransceiver
  - RTCPeerConnection.prototype.setLocalDescription

Purpose:
  - Capture WebRTC audio tracks
  - Optionally inject TTS audio into WebRTC stream
  - Monitor WebRTC connection state
```

---

## 12. Error Handling & Recovery

### 12.1 WebSocket Reconnection
```
STT/TTS WebSocket closes:
  â”œâ”€> Attempt reconnection (max 5 attempts)
  â”œâ”€> Delay: 3s between attempts
  â””â”€> If all fail â†’ Log error, continue with existing connection
```

### 12.2 Audio Capture Errors
```
Audio capture fails:
  â”œâ”€> Log error
  â”œâ”€> Attempt to restart capture
  â””â”€> If persistent â†’ Continue without capture (bot won't hear)
```

### 12.3 TTS Playback Errors
```
paplay fails:
  â”œâ”€> Fallback to WebRTC injection
  â””â”€> If both fail â†’ Log error, continue
```

---

## 13. Complete End-to-End Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INTERVIEW BOT FLOW                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

START
  â”‚
  â”œâ”€> Docker starts â†’ PulseAudio setup â†’ Browser launch
  â”‚
  â”œâ”€> Navigate to interview URL
  â”‚
  â”œâ”€> Inject automation script
  â”‚
  â”œâ”€> Auto-click: Get Started â†’ Password â†’ Language â†’ Start Interview
  â”‚
  â”œâ”€> Interview ready â†’ Start audio capture
  â”‚
  â””â”€> LOOP: Conversation
      â”‚
      â”œâ”€> [Umi speaks]
      â”‚   â”‚
      â”‚   â”œâ”€> Browser captures from virtual_mic
      â”‚   â”‚
      â”‚   â”œâ”€> AudioWorklet â†’ PCM â†’ Base64
      â”‚   â”‚
      â”‚   â”œâ”€> STT WebSocket â†’ Gemini API
      â”‚   â”‚
      â”‚   â”œâ”€> Gemini transcribes â†’ Returns text
      â”‚   â”‚
      â”‚   â””â”€> Conversation manager accumulates transcript
      â”‚
      â”œâ”€> [5s silence detected]
      â”‚   â”‚
      â”‚   â”œâ”€> Generate response prompt
      â”‚   â”‚
      â”‚   â”œâ”€> TTS WebSocket â†’ Gemini API
      â”‚   â”‚
      â”‚   â”œâ”€> Gemini generates audio response
      â”‚   â”‚
      â”‚   â”œâ”€> Receive base64 audio chunks
      â”‚   â”‚
      â”‚   â”œâ”€> Queue for playback
      â”‚   â”‚
      â”‚   â”œâ”€> paplay â†’ virtual_speaker
      â”‚   â”‚
      â”‚   â”œâ”€> PulseAudio loopback â†’ virtual_mic
      â”‚   â”‚
      â”‚   â”œâ”€> Browser WebRTC captures â†’ Umi hears
      â”‚   â”‚
      â”‚   â””â”€> 15s cooldown â†’ Ready for next question
      â”‚
      â””â”€> [Repeat until interview ends or timeout]
```

---

## 14. Key APIs Used

### 14.1 Google Gemini Live API
- **Endpoint**: `wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1alpha.GenerativeService.BidiGenerateContent`
- **Authentication**: Query parameter `?key=API_KEY`
- **Protocol**: WebSocket (bidirectional)
- **Models**:
  - STT: `models/gemini-2.0-flash-live-001` (TEXT response)
  - TTS: `models/gemini-2.0-flash-live-001` (AUDIO + TEXT response)
- **Voice**: `Puck` (prebuilt voice)

### 14.2 PulseAudio
- **virtual_speaker**: Sink for TTS playback
- **virtual_mic**: Source for browser capture (monitors virtual_speaker)
- **Commands**: `paplay`, `pactl`

### 14.3 Puppeteer
- **Browser**: Chromium (headless: false)
- **Page**: Single page for interview
- **API**: `page.goto()`, `page.evaluate()`, `page.exposeFunction()`

### 14.4 WebRTC
- **getUserMedia**: Capture audio from virtual_mic
- **RTCPeerConnection**: LiveKit WebRTC for interview audio
- **AudioTrack**: Microphone input track

---

## 15. Data Formats

### 15.1 Audio Format (Capture)
- **Format**: PCM (16-bit signed integer, little-endian)
- **Sample Rate**: 44100 Hz
- **Channels**: 1 (mono)
- **Chunk Duration**: 2000ms
- **Encoding**: Base64 for WebSocket transmission

### 15.2 Audio Format (Playback)
- **Format**: PCM (16-bit signed integer, little-endian)
- **Sample Rate**: 24000 Hz (from Gemini TTS)
- **Channels**: 1 (mono)
- **Encoding**: Base64 from Gemini API

### 15.3 WebSocket Messages
- **Format**: JSON
- **Setup**: `{ setup: { ... } }`
- **Audio**: `{ audio: { data: "<base64>", format: "pcm", sampleRate: 44100 } }`
- **Text**: `{ text: "..." }`
- **Response**: `{ serverContent: { ... } }`

---

## 16. Configuration

### 16.1 Environment Variables
```bash
INTERVIEW_URL=https://interview-staging.findarbor.com/interview/...
INTERVIEW_PASSWORD=...
GEMINI_API_KEY=...
TIMEOUT_SECONDS=3600
LOG_LEVEL=info
HEALTH_PORT=3000
```

### 16.2 Timing Configuration
- **Response Delay**: 5000ms (wait for silence before responding)
- **Cooldown**: 15000ms (wait after TTS playback)
- **Audio Chunk**: 2000ms
- **Auto-click Interval**: 2000ms

---

## 17. Troubleshooting Points

### 17.1 Audio Not Flowing
- Check PulseAudio: `pactl list sources short` â†’ Should see `virtual_mic`
- Check browser device: Logs should show "âœ“ Found virtual_mic"
- Check loopback: `pactl list modules short | grep loopback`

### 17.2 WebSocket Issues
- Check API key: Logs should show "API key length: 39"
- Check connection: Logs should show "WebSocket connected"
- Check setup: Logs should show "Setup complete"

### 17.3 TTS Not Playing
- Check paplay: Logs should show "PRIMARY: Playing via paplay"
- Check queue: Logs should show "playNextFromQueue called"
- Check PulseAudio sink: `pactl list sinks short` â†’ Should see `virtual_speaker`

---

## Summary

The interview bot orchestrates a complex real-time audio conversation:
1. **Captures** Umi's speech via browser WebRTC â†’ PulseAudio virtual_mic
2. **Transcribes** via Gemini STT WebSocket
3. **Generates** response via Gemini TTS WebSocket
4. **Plays** TTS via paplay â†’ PulseAudio virtual_speaker
5. **Loops back** audio via PulseAudio â†’ virtual_mic â†’ Browser WebRTC â†’ Umi hears

The entire flow is automated, with the bot handling turn-taking, silence detection, and audio routing to create a seamless interview experience.

